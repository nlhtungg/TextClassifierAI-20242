import joblib
import numpy as np
import re
import gensim
from sklearn.preprocessing import LabelEncoder

# T·∫£i m√¥ h√¨nh SVM v√† Word2Vec
svm_clf = joblib.load('model/svm_classifier_pipeline.pkl.gz')  # Load the gzipped SVM model
w2v_model = gensim.models.Word2Vec.load('model/word2vec_embedding.model')  # T·∫£i m√¥ h√¨nh Word2Vec
label_encoder = joblib.load('model/label_encoder.pkl')  # T·∫£i LabelEncoder

# Danh s√°ch stopwords, b·∫°n c√≥ th·ªÉ thay th·∫ø b·∫±ng danh s√°ch stopwords th·ª±c t·∫ø c·ªßa m√¨nh
stopwords = set(["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves"])


# H√†m d·ª± ƒëo√°n nh√£n cho vƒÉn b·∫£n
def predict_category(text, w2v_model, vector_size=300, clf=svm_clf):
    # L√†m s·∫°ch vƒÉn b·∫£n
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # B·ªè d·∫•u c√¢u
    tokens = text.split()
    tokens = [w for w in tokens if w not in stopwords]

    # Vector h√≥a b·∫±ng Word2Vec (trung b√¨nh vector c√°c t·ª´)
    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]
    if len(vectors) == 0:
        vector = np.zeros(vector_size).reshape(1, -1)
    else:
        vector = np.mean(vectors, axis=0).reshape(1, -1)

    # D·ª± ƒëo√°n nh√£n
    label_id = clf.predict(vector)[0]
    label_name = label_encoder.inverse_transform([label_id])[0]

    return label_name
if __name__ == "__main__":
    # Ki·ªÉm tra d·ª± ƒëo√°n
    test_sentence = "A bird flies on the sky"

    predicted_category = predict_category(test_sentence, w2v_model)
    print(f"üìù C√¢u: {test_sentence}")
    print(f"üìÇ D·ª± ƒëo√°n thu·ªôc nh√£n: {predicted_category}")
